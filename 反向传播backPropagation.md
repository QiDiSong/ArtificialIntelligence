# 反向传播算法

**反向传播**（英语：Backpropagation，缩写为**BP**）是“误差反向传播”的简称，是一种与[最优化方法](https://zh.wikipedia.org/wiki/最优化)（如[梯度下降法](https://zh.wikipedia.org/wiki/梯度下降法)）结合使用的，用来训练[人工神经网络](https://zh.wikipedia.org/wiki/人工神经网络)的常见方法。该方法对网络中所有权重计算[损失函数](https://zh.wikipedia.org/wiki/损失函数)的梯度。这个梯度会反馈给最优化方法，用来更新权值以最小化损失函数。

反向传播要求有对每个输入值想得到的已知输出，来计算损失函数梯度。因此，它通常被认为是一种[监督式学习](https://zh.wikipedia.org/wiki/監督式學習)方法，虽然它也用在一些[无监督](https://zh.wikipedia.org/wiki/非監督式學習)网络（如[自动编码器](https://zh.wikipedia.org/wiki/自编码器)）中。它是多层[前馈网络](https://zh.wikipedia.org/wiki/前馈神经网络)的[Delta规则](https://zh.wikipedia.org/w/index.php?title=Delta规则&action=edit&redlink=1)的推广，可以用[链式法则](https://zh.wikipedia.org/wiki/链式法则)对每层迭代计算梯度。反向传播要求[人工神经元](https://zh.wikipedia.org/w/index.php?title=人工神经元&action=edit&redlink=1)（或“节点”）的[激励函数](https://zh.wikipedia.org/wiki/激励函数)[可微](https://zh.wikipedia.org/wiki/可微函数)。

## 动机

任何[监督式学习](https://zh.wikipedia.org/wiki/監督式學習)算法的目标是找到一个能把一组输入最好地映射到其正确的输出的函数。例如一个简单的[分类](https://zh.wikipedia.org/wiki/分类问题)任务，其中输入是动物的图像，正确的输出将是动物的名称。一些输入和输出模式可以很容易地通过单层神经网络（如[感知器](https://zh.wikipedia.org/wiki/感知器)）学习。但是这些单层的感知机只能学习一些比较简单的模式，例如那些非[线性可分的](https://zh.wikipedia.org/w/index.php?title=线性可分的&action=edit&redlink=1)模式。例如，人可以通过识别动物的图像的某些特征进行分类，例如肢的数目，皮肤的纹理（无论是毛皮，羽毛，鳞片等），该动物的体型，以及种种其他特征。但是，单层神经网络必须仅仅使用图像中的像素的强度来学习一个输出一个标签函数。因为它被限制为仅具有一个层，所以没有办法从输入中学习到任何抽象特征。多层的网络克服了这一限制，因为它可以创建内部表示，并在每一层学习不同的特征。[[1\]](https://zh.wikipedia.org/wiki/反向传播算法#cite_note-Rumelhart1986-1) 第一层可能负责从图像的单个像素的输入学习线条的走向。第二层可能就会结合第一层所学并学习识别简单形状（如圆形）。每升高一层就学习越来越多的抽象特征，如上文提到的用来图像分类。每一层都是从它下方的层中找到模式，就是这种能力创建了独立于为多层网络提供能量的外界输入的内部表达形式。 反向传播算法的发展的目标和动机是找到一种训练的多层神经网络的方法，于是它可以学习合适的内部表达来让它学习任意的输入到输出的映射。[[1\]](https://zh.wikipedia.org/wiki/反向传播算法#cite_note-Rumelhart1986-1)

## 概括

反向传播算法（BP 算法）主要由两个阶段组成：激励传播与权重更新。

### 第1阶段：激励传播

每次迭代中的传播环节包含两步：

1. <u>（前向传播阶段）将训练输入送入网络以获得激励响应；</u>
2. <u>（反向传播阶段）将激励响应同训练输入对应的目标输出求差，从而获得输出层和隐藏层的响应误差。</u>

### 第2阶段：权重更新

对于每个突触上的权重，按照以下步骤进行更新：

1. <u>将输入激励和响应误差相乘，从而获得权重的梯度；</u>
2. <u>将这个梯度乘上一个比例并取反后加到权重上。</u>

这个比例（百分比）将会影响到训练过程的速度和效果，因此成为“**<u>训练因子</u>**”。<u>梯度的方向指明了误差扩大的方向，因此在更新权重的时候需要对其取反，从而减小权重引起的误差。</u>

<u>第 1 和第 2 阶段可以反复循环迭代，直到网络对输入的响应达到满意的预定的目标范围为止。</u>
